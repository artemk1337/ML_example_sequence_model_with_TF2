{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --no-check-certificate \\\n    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n    -O /tmp/sarcasm.json","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow_datasets as tfds\nimport json\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Example"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sentences = [\n    'I love dogs and dont love cats',\n    'Cats dont love dogs'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=None, oov_token='<OOV>')\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\nprint(tokenizer.word_index)\nprint(tokenizer.texts_to_sequences(sentences))\nprint(tokenizer.texts_to_sequences(['I love cars']))  ## unknown word cars -> <OOV> -> 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)\npadded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/tmp/sarcasm.json', 'r', encoding='utf-8') as f:\n    datastore = json.load(f)\n\nsentences = []\nlabels = []\nurls = []\nfor item in datastore:\n    sentences.append(item['headline'])\n    labels.append(item['is_sarcastic'])\n    urls.append(item['article_link'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(sentences)\n\nword_index = tokenizer.word_index\nprint(len(word_index))\n#print(word_index)\nsequences = tokenizer.texts_to_sequences(sentences)\npadded = pad_sequences(sequences, padding='post')\nprint(padded[0])\nprint(padded.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imdb"},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = imdb['train'], imdb['test']\ntrain_data, test_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sent, train_labels = [], []\ntest_sent, test_labels = [], []\n\nfor s, l in train_data:\n    train_sent.append(s.numpy().decode('UTF-8'))\n    train_labels += [l.numpy()]\n    \nfor s, l in test_data:\n    test_sent.append(s.numpy().decode('UTF-8'))\n    test_labels += [l.numpy()]\n\ntrain_sent[:2], train_labels[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\ntokenizer.fit_on_texts(train_sent)\ntrain_seq = tokenizer.texts_to_sequences(train_sent)\ntrain_padded = pad_sequences(train_seq, maxlen=120, truncating='post')\n\ntest_seq = tokenizer.texts_to_sequences(test_sent)\ntest_padded = pad_sequences(test_seq, maxlen=120, truncating='post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(10000, 16, input_length=120),\n        tf.keras.layers.LSTM(16, return_sequences=True),\n        tf.keras.layers.LSTM(16, return_sequences=True),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(8, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid'),\n    ])\n    return model\nmodel = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.fit(train_padded, np.array(train_labels), epochs=10, validation_data=(test_padded, np.array(test_labels)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = model.layers[0].get_weights()[0]\nweights.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import io\n\nreverse_word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])\n\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\nfor word_num in range(1, 10000):\n    word = reverse_word_index[word_num]\n    embeddings = weights[word_num]\n    out_m.write(word + \"\\n\")\n    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"I really think this is amazing. honest.\"\nsequence = tokenizer.texts_to_sequences([sentence])\nprint(sequence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Use generator instead of arrays\nhttps://www.tensorflow.org/tutorials/load_data/text#encode_text_lines_as_numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = tfds.features.text.Tokenizer()\n\nvocabulary_set = set()\nfor text_tensor, label in train_data:\n    some_tokens = tokenizer.tokenize(text_tensor.numpy().decode('UTF-8'))\n    vocabulary_set.update(some_tokens)\n\nvocab_size = len(vocabulary_set)\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\nexample_text = next(iter(train_data))[0].numpy()\nprint(example_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_example = encoder.encode(example_text)\nprint(encoded_example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder.decode(encoder.encode('Hello, Friend'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}